{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03882dc6",
   "metadata": {},
   "source": [
    "The notebook presents cell segmentation by integrating U-Net architecture. Data was taken from [here](https://www.kaggle.com/competitions/data-science-bowl-2018/data). Unfortunately, the project still lacks from success due to non-updated errors in the training, encouraging more investigation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efdf1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c008d581",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bd197d",
   "metadata": {},
   "source": [
    "Defining constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6461e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'stage1_train'\n",
    "TEST_PATH = 'stage1_test'\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n",
    "SIZE_IMAGE = (3, 270, 290)\n",
    "SIZE_MASK = (1, 194, 226)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e98d7b",
   "metadata": {},
   "source": [
    "Creating Dataset class to import data to notebook with data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "675522b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, filepath, device = 'cpu', transform_image = None, train = True, transform_mask = None,\n",
    "                 size_image = (128, 128), size_mask = (128, 128)):\n",
    "        self.device = device\n",
    "        self.filepath = filepath\n",
    "        self.filenames = os.listdir(self.filepath)\n",
    "        self.train = train\n",
    "        self.size_image = size_image\n",
    "        self.size_mask = size_mask\n",
    "        self.transform_image = transforms.Normalize\n",
    "        self.transform_mask = transforms.Normalize\n",
    "\n",
    "        # height, width\n",
    "        self.resizer_image = transforms.Resize(size_image)\n",
    "        self.resizer_mask = transforms.Resize(size_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.filepath, self.filenames[index], 'images', self.filenames[index] + '.png')\n",
    "\n",
    "        image = read_image(img_path)[:3, :, :].to(self.device)\n",
    "        image = image.float()\n",
    "        image = image / 255\n",
    "        image = self.resizer_image(image)\n",
    "\n",
    "        if self.train:\n",
    "            mask_path = os.path.join(self.filepath, self.filenames[index], 'masks')\n",
    "            mask = torch.zeros((1, *self.size_mask)).to(self.device)\n",
    "            for filename in os.listdir(mask_path):\n",
    "                mask_file_path = os.path.join(mask_path, filename)\n",
    "\n",
    "                mask_image = read_image(mask_file_path).to(self.device)\n",
    "                mask_image = self.resizer_mask(mask_image)\n",
    "                mask = torch.maximum(mask, mask_image)\n",
    "\n",
    "            mask = mask / 255\n",
    "\n",
    "            return image, mask\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6fc1b",
   "metadata": {},
   "source": [
    "Creating the dataset object. It is worth to add that the train data was previously and randomly splitted inside stage1_train and 10% of the data was collected in a new folder called stage1_val to evaluate the model performance later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02204051",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = ImageDataset('stage1_train', train = True, \n",
    "                                size_image = SIZE_IMAGE[1:], size_mask = SIZE_MASK[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "353bafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = ImageDataset('stage1_val', train = True, \n",
    "                                size_image = SIZE_IMAGE[1:], size_mask = SIZE_MASK[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec8dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageDataset('stage1_test', train = False, \n",
    "                                size_image = SIZE_IMAGE[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3f7042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(training_dataset, batch_size = 16)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size = 16)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec99ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21cec333",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54135e2",
   "metadata": {},
   "source": [
    "Creating U-Net architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "820ada16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, image_shape, kernel_size_conv = 3, kernel_size_pool = 2,\n",
    "                 kernel_size_conv_transpose = 2, conv_padding = 0,\n",
    "                 stride = 2, pool_padding = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        # dropout\n",
    "        c, h, w = image_shape\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = 0.1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = kernel_size_pool, stride = stride, padding = pool_padding)\n",
    "\n",
    "        self.conv_c1_c = nn.Conv2d(in_channels = c, out_channels = 16, kernel_size = kernel_size_conv, padding = conv_padding)\n",
    "        self.conv_c1 = nn.Conv2d(in_channels = 16, out_channels = 16, kernel_size = kernel_size_conv, padding = conv_padding)\n",
    "\n",
    "        self.conv_c2_1 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = kernel_size_conv, padding = conv_padding)\n",
    "        self.conv_c2_2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = kernel_size_conv, padding = conv_padding)\n",
    "\n",
    "        self.conv_c3_1 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = kernel_size_conv, padding = conv_padding)\n",
    "        self.conv_c3_2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = kernel_size_conv, padding = conv_padding)\n",
    "\n",
    "        self.conv_c4_1 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = kernel_size_conv, padding = conv_padding)\n",
    "        self.conv_c4_2 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = kernel_size_conv, padding = conv_padding)\n",
    "\n",
    "        self.conv_c5_1 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = kernel_size_conv, padding = conv_padding)\n",
    "        self.conv_c5_2 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = kernel_size_conv, padding = conv_padding)\n",
    "\n",
    "        self.conv_u1_1 = nn.Conv2d(in_channels = 32, out_channels = 16, kernel_size = kernel_size_conv)\n",
    "        self.conv_u1_2 = nn.Conv2d(in_channels = 16, out_channels = 16, kernel_size = kernel_size_conv)\n",
    "        self.conv_u1_3 = nn.Conv2d(in_channels = 16, out_channels = 1, kernel_size = kernel_size_conv)\n",
    "        self.conv_transpose_u1 = nn.ConvTranspose2d(in_channels = 32, out_channels = 16, stride = stride, kernel_size = kernel_size_conv_transpose)\n",
    "\n",
    "        self.conv_u2_1 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = kernel_size_conv)\n",
    "        self.conv_u2_2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = kernel_size_conv)\n",
    "        self.conv_transpose_u2 = nn.ConvTranspose2d(in_channels = 64, out_channels = 32, stride = stride, kernel_size = kernel_size_conv_transpose)\n",
    "\n",
    "        self.conv_u3_1 = nn.Conv2d(in_channels = 128, out_channels = 64, kernel_size = kernel_size_conv)\n",
    "        self.conv_u3_2 = nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = kernel_size_conv)\n",
    "        self.conv_transpose_u3 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, stride = stride, kernel_size = kernel_size_conv_transpose)\n",
    "\n",
    "        self.conv_u4_1 = nn.Conv2d(in_channels = 256, out_channels = 128, kernel_size = kernel_size_conv)\n",
    "        self.conv_u4_2 = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = kernel_size_conv)\n",
    "        self.conv_transpose_u4 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, stride = stride, kernel_size = kernel_size_conv_transpose)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xc1 = self.conv_c1_c(x)\n",
    "        xc1 = self.relu(xc1)\n",
    "        xc1 = self.dropout(xc1)\n",
    "        xc1 = self.conv_c1(xc1)\n",
    "        xc1 = self.relu(xc1)\n",
    "        xc2 = self.pool(xc1)\n",
    "\n",
    "        xc2 = self.conv_c2_1(xc2)\n",
    "        xc2 = self.relu(xc2)\n",
    "        xc2 = self.dropout(xc2)\n",
    "        xc2 = self.conv_c2_2(xc2)\n",
    "        xc2= self.relu(xc2)\n",
    "        xc3 = self.pool(xc2)\n",
    "\n",
    "        xc3 = self.conv_c3_1(xc3)\n",
    "        xc3 = self.relu(xc3)\n",
    "        xc3 = self.dropout(xc3)\n",
    "        xc3 = self.conv_c3_2(xc3)\n",
    "        xc3 = self.relu(xc3)\n",
    "        xc4 = self.pool(xc3)\n",
    "\n",
    "        xc4 = self.conv_c4_1(xc4)\n",
    "        xc4 = self.relu(xc4)\n",
    "        xc4 = self.dropout(xc4)\n",
    "        xc4 = self.conv_c4_2(xc4)\n",
    "        xc4 = self.relu(xc4)\n",
    "        xc5 = self.pool(xc4)\n",
    "\n",
    "        xc5 = self.conv_c5_1(xc5)\n",
    "        xc5 = self.relu(xc5)\n",
    "        xc5 = self.dropout(xc5)\n",
    "        xc5 = self.conv_c5_2(xc5)\n",
    "        xc5 = self.relu(xc5)\n",
    "\n",
    "        xu4 = self.conv_transpose_u4(xc5)\n",
    "\n",
    "        k_dim_2 = (xc4.shape[2] - xu4.shape[2]) // 2\n",
    "        k_residual_dim_2 = (xc4.shape[2] - xu4.shape[2]) % 2\n",
    "        k_dim_3 = (xc4.shape[3] - xu4.shape[3]) // 2\n",
    "        k_residual_dim_3 = (xc4.shape[3] - xu4.shape[3]) % 2\n",
    "        size_dim_2 = xc4.shape[2] - k_residual_dim_2\n",
    "        size_dim_3 = xc4.shape[3] - k_residual_dim_3\n",
    "\n",
    "        xc4_portion = xc4[:, :, k_dim_2:size_dim_2 - k_dim_2, k_dim_3:size_dim_3 - k_dim_3]\n",
    "        xu4 = torch.cat((xc4_portion, xu4), dim = 1)\n",
    "        xu4 = self.conv_u4_1(xu4)\n",
    "        xu4 = self.relu(xu4)\n",
    "        xu4 = self.dropout(xu4)\n",
    "        xu4 = self.conv_u4_2(xu4)\n",
    "        xu4 = self.relu(xu4)\n",
    "\n",
    "        xu3 = self.conv_transpose_u3(xu4)\n",
    "\n",
    "        k_dim_2 = (xc3.shape[2] - xu3.shape[2]) // 2\n",
    "        k_residual_dim_2 = (xc3.shape[2] - xu3.shape[2]) % 2\n",
    "        k_dim_3 = (xc3.shape[3] - xu3.shape[3]) // 2\n",
    "        k_residual_dim_3 = (xc3.shape[3] - xu3.shape[3]) % 2\n",
    "        size_dim_2 = xc3.shape[2] - k_residual_dim_2\n",
    "        size_dim_3 = xc3.shape[3] - k_residual_dim_3\n",
    "\n",
    "        xc3_portion = xc3[:, :, k_dim_2:size_dim_2 - k_dim_2, k_dim_3:size_dim_3 - k_dim_3]\n",
    "        xu3 = torch.cat((xc3_portion, xu3), dim = 1)\n",
    "        xu3 = self.conv_u3_1(xu3)\n",
    "        xu3 = self.relu(xu3)\n",
    "        xu3 = self.dropout(xu3)\n",
    "        xu3 = self.conv_u3_2(xu3)\n",
    "        xu3 = self.relu(xu3)\n",
    "\n",
    "        xu2 = self.conv_transpose_u2(xu3)\n",
    "\n",
    "        k_dim_2 = (xc2.shape[2] - xu2.shape[2]) // 2\n",
    "        k_residual_dim_2 = (xc2.shape[2] - xu2.shape[2]) % 2\n",
    "        k_dim_3 = (xc2.shape[3] - xu2.shape[3]) // 2\n",
    "        k_residual_dim_3 = (xc2.shape[3] - xu2.shape[3]) % 2\n",
    "        size_dim_2 = xc2.shape[2] - k_residual_dim_2\n",
    "        size_dim_3 = xc2.shape[3] - k_residual_dim_3\n",
    "\n",
    "        xc2_portion = xc2[:, :, k_dim_2:size_dim_2 - k_dim_2, k_dim_3:size_dim_3 - k_dim_3]\n",
    "        xu2 = torch.cat((xc2_portion, xu2), dim = 1)\n",
    "        xu2 = self.conv_u2_1(xu2)\n",
    "        xu2 = self.relu(xu2)\n",
    "        xu2 = self.dropout(xu2)\n",
    "        xu2 = self.conv_u2_2(xu2)\n",
    "        xu2 = self.relu(xu2)\n",
    "\n",
    "        xu1 = self.conv_transpose_u1(xu2)\n",
    "\n",
    "        k_dim_2 = (xc1.shape[2] - xu1.shape[2]) // 2\n",
    "        k_residual_dim_2 = (xc1.shape[2] - xu1.shape[2]) % 2\n",
    "        k_dim_3 = (xc1.shape[3] - xu1.shape[3]) // 2\n",
    "        k_residual_dim_3 = (xc1.shape[3] - xu1.shape[3]) % 2\n",
    "        size_dim_2 = xc1.shape[2] - k_residual_dim_2\n",
    "        size_dim_3 = xc1.shape[3] - k_residual_dim_3\n",
    "\n",
    "        xc1_portion = xc1[:, :, k_dim_2:size_dim_2 - k_dim_2, k_dim_3:size_dim_3 - k_dim_3]\n",
    "        xu1 = torch.cat((xc1_portion, xu1), dim = 1)\n",
    "        xu1 = self.conv_u1_1(xu1)\n",
    "        xu1 = self.relu(xu1)\n",
    "        xu1 = self.dropout(xu1)\n",
    "        xu1 = self.conv_u1_2(xu1)\n",
    "        xu1 = self.relu(xu1)\n",
    "        xu1 = self.conv_u1_3(xu1)\n",
    "        xu1 = self.sigmoid(xu1)\n",
    "        \n",
    "        return xu1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3492f",
   "metadata": {},
   "source": [
    "Initializing model and checking the random performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d694874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(SIZE_IMAGE, conv_padding = 'same').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ec842d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 194, 226])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((16, 3, 270, 290))\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6712aa5",
   "metadata": {},
   "source": [
    "Number of total parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c7b356a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(model.parameters())\n",
    "\n",
    "total_parameters = 0\n",
    "\n",
    "for subparameters in iterator:\n",
    "    total_parameters += subparameters.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aeaaa2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941233"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab64752",
   "metadata": {},
   "source": [
    "Defining the optimizer, loss function, train and evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3addde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eb735062",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "31afde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, loss_fn, n):\n",
    "    model.train()\n",
    "    m = 5\n",
    "    train_loss = 0\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        print('Prediction')\n",
    "        pred = model(X)\n",
    "        \n",
    "        loss = loss_fn(y, pred)\n",
    "        \n",
    "        print('Backwarding\\n')\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if i % m == m - 1:\n",
    "            print('{} Epoch --> {}/{} with loss of {:.5f}'.format(\n",
    "                    epoch + 1, i + 1, n, train_loss / m \n",
    "                    ))\n",
    "            total_train_loss += train_loss\n",
    "            train_loss = 0\n",
    "    \n",
    "    if i % m != m - 1:   \n",
    "        total_train_loss += train_loss\n",
    "    return total_train_loss / n\n",
    "            \n",
    "def evaluate_model(model, validation_loader, loss_fn, n):\n",
    "    model.eval()\n",
    "    m = 5\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for i, (X, y) in enumerate(validation_loader):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            if i % m == m - 1:\n",
    "                print(i)\n",
    "                \n",
    "        return total_val_loss / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457d79b",
   "metadata": {},
   "source": [
    "Training and Testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1d5363ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "1 Epoch --> 5/38 with loss of 49.51272\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "1 Epoch --> 10/38 with loss of 49.56812\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "1 Epoch --> 15/38 with loss of 49.38841\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "1 Epoch --> 20/38 with loss of 49.34517\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "1 Epoch --> 25/38 with loss of 49.69519\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "1 Epoch --> 30/38 with loss of 49.57729\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "1 Epoch --> 35/38 with loss of 49.43976\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "\n",
      "Average train loss error --> 49.50554\n",
      "4\n",
      "Average test loss error --> 0.71213\n",
      "\n",
      "--------------------------------------------------\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "2 Epoch --> 5/38 with loss of 49.51266\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "2 Epoch --> 10/38 with loss of 49.56814\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "2 Epoch --> 15/38 with loss of 49.38844\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "2 Epoch --> 20/38 with loss of 49.34514\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "2 Epoch --> 25/38 with loss of 49.69523\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "2 Epoch --> 30/38 with loss of 49.57725\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "2 Epoch --> 35/38 with loss of 49.43978\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "\n",
      "Average train loss error --> 49.50554\n",
      "4\n",
      "Average test loss error --> 0.71213\n",
      "\n",
      "--------------------------------------------------\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "3 Epoch --> 5/38 with loss of 49.51265\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "3 Epoch --> 10/38 with loss of 49.56816\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "3 Epoch --> 15/38 with loss of 49.38849\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "3 Epoch --> 20/38 with loss of 49.34514\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "3 Epoch --> 25/38 with loss of 49.69523\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "3 Epoch --> 30/38 with loss of 49.57722\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "3 Epoch --> 35/38 with loss of 49.43984\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "\n",
      "Average train loss error --> 49.50555\n",
      "4\n",
      "Average test loss error --> 0.71213\n",
      "\n",
      "--------------------------------------------------\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "4 Epoch --> 5/38 with loss of 49.51277\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "4 Epoch --> 10/38 with loss of 49.56801\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "4 Epoch --> 15/38 with loss of 49.38841\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "4 Epoch --> 20/38 with loss of 49.34515\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "4 Epoch --> 25/38 with loss of 49.69520\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "4 Epoch --> 30/38 with loss of 49.57729\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "4 Epoch --> 35/38 with loss of 49.43979\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "\n",
      "Average train loss error --> 49.50554\n",
      "4\n",
      "Average test loss error --> 0.71213\n",
      "\n",
      "--------------------------------------------------\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "5 Epoch --> 5/38 with loss of 49.51269\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "5 Epoch --> 10/38 with loss of 49.56819\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "5 Epoch --> 15/38 with loss of 49.38841\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "5 Epoch --> 20/38 with loss of 49.34507\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "5 Epoch --> 25/38 with loss of 49.69527\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "5 Epoch --> 30/38 with loss of 49.57730\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "5 Epoch --> 35/38 with loss of 49.43983\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "Prediction\n",
      "Backwarding\n",
      "\n",
      "\n",
      "Average train loss error --> 49.50555\n",
      "4\n",
      "Average test loss error --> 0.71213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('-' * 50)\n",
    "    \n",
    "    train_loss = train_model(model, train_loader, optimizer, loss_fn, n = len(train_loader))\n",
    "    print('\\nAverage train loss error --> {:.5f}'.format(train_loss))\n",
    "    \n",
    "    test_loss = evaluate_model(model, validation_loader, loss_fn, n = len(validation_loader))\n",
    "    print('Average test loss error --> {:.5f}\\n'.format(test_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f92b2",
   "metadata": {},
   "source": [
    "Saving the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "72f6c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/UNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2e7487d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(SIZE_IMAGE).to('cpu')\n",
    "model.load_state_dict(torch.load('./models/UNet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156eb8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e014fc81",
   "metadata": {},
   "source": [
    "The project is still under the development since errors in each corresponding iteration of epochs seem not be updated properly. This suggests that further investigation is necessary and final and more successfuly project is expected to be uploaded within 1-2 weeks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
